---
title: "COVID State Web Scraping"
output: html_document
---

# Set Up

```{r, message = FALSE, warning = FALSE }
library(tidyverse)
library(pdftools)
library(rvest)
library(lubridate)
library(readxl)
source("states.R")
```


# PDF/CSV States

Notes To Self:

- Oklahoma: Demographic and age data too messy to parse cleanly
- Mississippi: Age and sex figures shown, but are contained in an image, not text
- Florida: function downloads the first pdf that it sees off of the page. It can take a while to work since Florida's pdfs are 600+ pages long
- Tennessee: Everything in xlsx, easier to scrape, need to input *date*
- North Carolina: missing age data since its in messy graphic
- District of Columbia: no information on age
- South Carolina: age and sex stored in messy Tableau visualization

## Oklahoma

```{r}
oklahoma = get_oklahoma()
oklahoma
```

## Mississippi

```{r}
mississippi = get_mississippi()
mississippi
```

## Florida

```{r}
# Changing columns to fit specific age brackets
fl_cols = c(
    "total",
    "age_0_4", 
    "age_5_14", 
    "age_15_24", 
    "age_25_34",
    "age_35_44", 
    "age_45_54", 
    "age_55_64", 
    "age_65_74", 
    "age_75_84", 
    "age_85+", 
    "age_unk",
    "sex_male", 
    "sex_female", 
    "sex_unk",
    "ethnicity_hispanic", 
    "ethnicity_non_hispanic",
    "ethnicity_unk",
    "race_white", 
    "race_AfrA",
    "race_NatA",
    "race_asian", 
    "race_other",
    "race_multi", 
    "race_unk")

florida = get_florida()
florida
```

## Tennessee

```{r}
tn_cols = c(
    "total",
    "age_0_10", 
    "age_11_20", 
    "age_21_30",
    "age_31_40",
    "age_41_50",
    "age_51_60",
    "age_61_70",
    "age_71_80",
    "age_81+", 
    "age_unk",
    "sex_male", 
    "sex_female", 
    "sex_unk",
    "ethnicity_hispanic", 
    "ethnicity_non_hispanic",
    "ethnicity_unk",
    "race_white", 
    "race_AfrA",
    "race_NatA",
    "race_asian", 
    "race_other",
    "race_multi", 
    "race_unk")

tennessee = get_tennessee("2020-05-12")
tennessee
```

## North Carolina

```{r}
north_carolina = get_north_carolina()
north_carolina
```

## District of Columbia

```{r}
dc = get_dc("May-10-2020")
dc
```


## South Carolina

```{r}
south_carolina = get_south_carolina()
south_carolina
```

## New Jersey

```{r}
# Need to update New Jersey
# Changing columns to account for provided data
nj_cols = c(
    "total",
    "age_0_4", 
    "age_5_17", 
    "age_18_29", 
    "age_30_49",
    "age_50_64", 
    "age_65_79", 
    "age_80+", 
    "age_unk",
    "sex_male", 
    "sex_female", 
    "sex_unk",
    "ethnicity_hispanic", 
    "ethnicity_non_hispanic",
    "ethnicity_unk",
    "race_white", 
    "race_AfrA",
    "race_NatA",
    "race_asian", 
    "race_other",
    "race_multi", 
    "race_unk")

new_jersey = get_new_jersey()
new_jersey
```

## New Hampshire

```{r}
nh_cols = c(
    "total",
    "age_0_9", 
    "age_10_19",
    "age_20_29", 
    "age_30_39", 
    "age_40_49",
    "age_50_59", 
    "age_60_69", 
    "age_70_79", 
    "age_80+", 
    "age_unk",
    "sex_male", 
    "sex_female", 
    "sex_unk",
    "ethnicity_hispanic", 
    "ethnicity_non_hispanic",
    "ethnicity_unk",
    "race_white", 
    "race_AfrA",
    "race_NatA",
    "race_asian", 
    "race_other",
    "race_multi", 
    "race_unk")

new_hampshire = get_new_hampshire()
new_hampshire 
```

## Guam

```{r}
guam = get_guam()
guam
```

## Virgin Island

- Data contained in a pdf, but the numbers are all over the place

## Puerto Rico

```{r}
# Must download data directly from site
# Site doesn't give a way to scrape for the csv itself
# Just going to use the default age cols from Aijin's first tab
get_puerto_rico = function(date) {
  
  
  # Actually wait this data is not updated with what I see in their graphs
  data = read_csv(paste0("../pdfs/puerto_rico_", date, ".csv"))
  
  return(data)
}

puerto_rico = get_puerto_rico("20200509")
puerto_rico
```

# Compile Data

## TODO:

- fix guam so that it searches for the first post about the updated figures
- combine Aijins and your data compilation into one
- standardize Aijins data into your format
- ask to just incorporate courtneys functions into your own compiler
- put everything into a source folder to get it all at once
- put all 50 state compilation code into one file to run
- create a function to make it easier to manually insert data from states that we can get all 50

ideas for compiling all 50 states
- PROGRESS BAR TO MAKE IT EASIER TO TRACK WHERE things go wrong
- automate the states that can be automated
- have other function make it easier to manually input data for other states (at least the base information for Lauren's 3x3)

```{r}
get_cdc = function() {
  url = "https://www.cdc.gov/TemplatePackage/contrib/widgets/cdcCharts/iframe.html?chost=www.cdc.gov&amp;cpath=/coronavirus/2019-ncov/cases-updates/cases-in-us.html&amp;csearch=&amp;chash=&amp;ctitle=Cases%20in%20the%20U.S.%20%7C%20CDC&amp;wn=cdcCharts&amp;wf=/TemplatePackage/contrib/widgets/cdcCharts/&amp;wid=cdcCharts3&amp;mMode=widget&amp;mPage=&amp;mChannel=&amp;host=www.cdc.gov&amp;displayMode=wcms&amp;configUrl=/coronavirus/2019-ncov/cases-updates/new_cases_by_day.json&amp;dataUrl=/coronavirus/2019-ncov/json/new-cases-chart-data.json&amp;class=mb-3"
  
  data = read_html(url) %>% 
    html_nodes("body")
  
  return(data)
}

new_add <- function(x, y, ...) {
  current_sum = x + y
  
  for (num in list(...)) { 
    current_sum = current_sum + num
  }
  
  return(current_sum)
}
new_add(1, 2, z = 3)
```



```{r}
# Just shove everything together
compile = function(tn_date, dc_date) {
  oklahoma = get_oklahoma()
  mississippi = get_mississippi()
  florida = get_florida()
  tennessee = get_tennessee("2020-05-11")
  north_carolina = get_north_carolina()
  dc = get_dc("May-10-2020")
  new_jersey = get_new_jersey()
  south_carolina = get_south_carolina()
  new_hampshire = get_new_hampshire()
  # guam = get_guam(url2)
  
  return(bind_rows(
    oklahoma,
    mississippi,
    florida,
    tennessee,
    north_carolina,
    dc,
    new_jersey,
    south_carolina,
    new_hampshire
  ))
}

compiled_data_20200512 = compile(
  tn_date = "2020-05-12",
  dc_date = "May-11-2020")
save(compiled_data_20200512, file = "../Data/meta_2020-05-12-cbp.rda")
```



# Tableau States

Notes To Self:

- Not possible/difficult to scrape the data off of the site. Best strategy seems to be manually downloading a pdf version of the necessary pages and scraping that instead.
- Iowa: demographic & age data available, but are messily formatted on the given pdf
- Arizona: different information is stored on different dashboards, so multiple pdfs need to be downloaded to get all the necessary data (Summary, Demographics)

## Iowa

```{r}
# Need to access the data through daily pdfs
# Get pdfs from Tableau download: https://coronavirus.iowa.gov/#CurrentStatus

now = Sys.time() %>% as_date() %>% as.character()
date_for_url = paste0(str_sub(now, 6,7), 
                      str_sub(now, 9, 10), 
                      str_sub(now, 1,4))

iowa = pdf_text(paste0("../pdfs/IowaCOVID19_", date_for_url, ".pdf"))

get_iowa = function(pdf_data) {
  total_page = pdf_data[1] %>% str_split(., "\n") %>% .[[1]]
  demographics_page = pdf_data[2] %>% str_split(., "\n") %>% .[[1]]
  
  total_confirmed_cases = total_page[35] %>% 
    str_split(" ") %>% .[[1]] %>% head(1) %>% 
    str_replace(., ",", "") %>% as.numeric()
  
  total_deaths = total_page[35] %>% 
    str_split(" ") %>% .[[1]] %>% tail(1) %>% 
    str_replace(., ",", "") %>% as.numeric()
  
  total_people_tested = total_page[37] %>% 
    str_split(" ") %>% .[[1]] %>% head(1) %>% 
    str_replace(., ",", "") %>% as.numeric()
  
  total_cases_recovered = total_page[37] %>% 
    str_split(" ") %>% .[[1]] %>% tail(1) %>% 
    str_replace(., ",", "") %>% as.numeric()
  
  
  return(list(
    total_confirmed_cases = total_confirmed_cases,
    total_deaths = total_deaths,
    total_people_tested = total_people_tested,
    total_cases_recovered = total_cases_recovered,
    demographic_string = demographic_page
  ))
}


iowa = get_iowa(iowa)
iowa
```

## Arizona

```{r}
# Link to Summary Dashboard
# https://tableau.azdhs.gov/views/COVID-19Summary/Overview2?:embed=y&amp;:showVizHome=no&amp;:host_url=https%3A%2F%2Ftableau.azdhs.gov%2F&amp;:embed_code_version=3&amp;:tabs=no&amp;:toolbar=no&amp;:showAppBanner=false&amp;:display_spinner=no&amp;iframeSizedToWindow=true&amp;:loadOrderID=1

# Link to Demographics Dashboard
# https://tableau.azdhs.gov/views/COVID19Demographics/EpiData?:embed=y&amp;:showVizHome=no&amp;:host_url=https%3A%2F%2Ftableau.azdhs.gov%2F&amp;:embed_code_version=3&amp;:tabs=no&amp;:toolbar=no&amp;:showAppBanner=false&amp;:display_spinner=no&amp;iframeSizedToWindow=true&amp;:loadOrderID=1

az_cols = c(
    "total",
    "age_0_19", 
    "age_20_44", 
    "age_45_54",
    "age_55_64", 
    "age_65+", 
    "age_unk",
    "sex_male", 
    "sex_female", 
    "sex_unk",
    "ethnicity_hispanic", 
    "ethnicity_non_hispanic",
    "ethnicity_unk",
    "race_white", 
    "race_AfrA",
    "race_NatA",
    "race_asian", 
    "race_other",
    "race_multi", 
    "race_unk")

get_arizona = function(date) {
  
  skeleton = skeleton_table(az_cols)
  
  summary_path = paste0("../pdfs/AZ/summary_", date, ".pdf")
  demo_path = paste0("../pdfs/AZ/demo_", date, ".pdf")
  
  summary_data = pdf_text(summary_path) %>% .[1] %>% 
    str_split("\n") %>% .[[1]] %>% 
    str_squish() %>% .[11] %>% 
    str_replace(", ", "") %>% str_replace(",", "") %>% 
    str_split(" ") %>% .[[1]] %>% .[2:4] %>% 
    as.numeric()
    
  skeleton[["cases"]][["total"]] = summary_data[1]
  skeleton[["deaths"]][["total"]] = summary_data[2]
  skeleton[["tested"]][["total"]] = summary_data[3]
  
  # Way too messy to be properly scraped but I'll leave this here
  demo_data = pdf_text(demo_path) %>% .[1] %>% 
    str_split("\n") %>% .[[1]] %>% 
    str_squish()
  
  return(list(
    data = as_tibble(skeleton),
    comments = list(
      "Demographic and age information are messily formatted for scraping"
    )
    ))
}

arizona = get_arizona("20200509")
arizona
```

# Microsoft BI States

## Nevada

- Doesn't work out because data is hidden under the Microsoft BI dashboard and the pdfs that hold the data show images, not text

```{r}
nevada = read_html("https://nvhealthresponse.nv.gov/") %>% 
    html_nodes(".js-side-cta") %>% 
    html_nodes(".split-content-section") %>% 
    html_nodes(".split-content-section__container") %>% 
    html_nodes(".content-intro") %>% 
    html_nodes("iframe") %>% 
    xml_attrs() %>% 
    .[[1]] %>% 
    .[1] %>% # This returns another html that comes from the iframe
    read_html() %>% 
    html_nodes("#pbiAppPlaceHolder")
  
  return(nevada)
```

## West Virginia

:( Microsoft BI

# ARCGIS?

## Delaware

```{r}
delaware = read_html("https://myhealthycommunity.dhss.delaware.gov/locations/state")
```

## Hawaii

Saw an ARCGIS link